/**
 * ğŸ†• v6.0: å¤šAIæ¨¡å‹ç»Ÿä¸€è°ƒç”¨å™¨
 * æ”¯æŒOpenAIã€DeepSeekã€Mistralã€Perplexityç­‰å¤šä¸ªæ¨¡å‹
 * æä¾›æ™ºèƒ½è·¯ç”±ã€é™çº§æœºåˆ¶å’Œæˆæœ¬è¿½è¸ª
 */

const fetch = require('node-fetch');

class MultiAIProvider {
  constructor() {
    // APIå¯†é’¥é…ç½®
    this.apiKeys = {
      openai: process.env.OPENAI_API_KEY,
      deepseek: process.env.DEEPSEEK_API_KEY,
      mistral: process.env.MISTRAL_API_KEY,
      perplexity: process.env.PERPLEXITY_API_KEY,
      anthropic: process.env.ANTHROPIC_API_KEY, // å¾…é…ç½®
      google: process.env.GOOGLE_API_KEY // å¾…é…ç½®
    };

    // æ¨¡å‹é…ç½®
    this.models = {
      // OpenAIç³»åˆ—ï¼ˆå·²æœ‰ï¼‰
      'gpt-4o': {
        provider: 'openai',
        endpoint: 'https://api.openai.com/v1/chat/completions',
        costPer1kTokens: { input: 0.0025, output: 0.01 },
        maxTokens: 128000,
        features: ['é€šç”¨åˆ†æ', 'è‹±æ–‡ä¼˜å…ˆ', 'å¤šæ¨¡æ€']
      },
      'gpt-4o-mini': {
        provider: 'openai',
        endpoint: 'https://api.openai.com/v1/chat/completions',
        costPer1kTokens: { input: 0.00015, output: 0.0006 },
        maxTokens: 128000,
        features: ['å¿«é€Ÿå“åº”', 'æˆæœ¬ä¼˜åŒ–']
      },
      
      // DeepSeek V3ï¼ˆä¸­æ–‡è´¢ç»ä¸“å®¶ï¼‰
      'deepseek-chat': {
        provider: 'deepseek',
        endpoint: 'https://api.deepseek.com/v1/chat/completions',
        costPer1kTokens: { input: 0.00027, output: 0.0011 },
        maxTokens: 64000,
        features: ['ä¸­æ–‡è´¢ç»', 'Aè‚¡åˆ†æ', 'æœ¬åœŸåŒ–æœ¯è¯­', 'æˆæœ¬æä½']
      },
      
      // Claude 3.5 Sonnetï¼ˆé•¿æ–‡æ·±åº¦åˆ†æï¼‰
      'claude-3-5-sonnet': {
        provider: 'anthropic',
        endpoint: 'https://api.anthropic.com/v1/messages',
        costPer1kTokens: { input: 0.003, output: 0.015 },
        maxTokens: 200000,
        features: ['é•¿æ–‡åˆ†æ', 'æ·±åº¦ç ”ç©¶', 'é€»è¾‘æ¨ç†', 'ä»£ç ç†è§£']
      },
      
      // Gemini 2.5 Flashï¼ˆå¿«é€Ÿæ‘˜è¦ï¼‰
      'gemini-2.5-flash': {
        provider: 'google',
        endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent',
        costPer1kTokens: { input: 0.00001, output: 0.00004 },
        maxTokens: 1000000,
        features: ['è¶…é•¿ä¸Šä¸‹æ–‡', 'å¿«é€Ÿæ‘˜è¦', 'å¤šæ¨¡æ€', 'æˆæœ¬æä½']
      },
      
      // Mistral Largeï¼ˆå¿«é€Ÿæ¨ç†ï¼‰
      'mistral-large-latest': {
        provider: 'mistral',
        endpoint: 'https://api.mistral.ai/v1/chat/completions',
        costPer1kTokens: { input: 0.002, output: 0.006 },
        maxTokens: 128000,
        features: ['å¿«é€Ÿæ¨ç†', 'å¤šè¯­è¨€', 'å‡½æ•°è°ƒç”¨']
      },
      
      // Perplexityï¼ˆå®æ—¶æœç´¢å¢å¼ºï¼‰
      'sonar-pro': {
        provider: 'perplexity',
        endpoint: 'https://api.perplexity.ai/chat/completions',
        costPer1kTokens: { input: 0.001, output: 0.001 },
        maxTokens: 127000,
        features: ['å®æ—¶æœç´¢', 'å¼•ç”¨æ¥æº', 'æœ€æ–°ä¿¡æ¯']
      }
    };

    // æ™ºèƒ½è·¯ç”±è§„åˆ™
    this.routingRules = {
      'chinese_analysis': 'deepseek-chat', // ä¸­æ–‡åˆ†æä¼˜å…ˆDeepSeek
      'quick_response': 'mistral-large-latest', // å¿«é€Ÿå“åº”ç”¨Mistral
      'real_time_search': 'sonar-pro', // å®æ—¶ä¿¡æ¯ç”¨Perplexity
      'long_context': 'gpt-4o', // é•¿æ–‡åˆ†æç”¨GPT-4o
      'default': 'gpt-4o-mini' // é»˜è®¤ç”¨æœ€ç»æµçš„æ¨¡å‹
    };

    // æˆæœ¬è¿½è¸ª
    this.costTracking = {
      totalCalls: 0,
      totalTokens: { input: 0, output: 0 },
      totalCost: 0,
      byModel: {}
    };
  }

  /**
   * æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹å’Œä¸Šä¸‹æ–‡é€‰æ‹©æœ€ä½³æ¨¡å‹
   * @param {string} taskType - ä»»åŠ¡ç±»å‹ï¼ˆchinese_analysis, quick_responseç­‰ï¼‰
   * @param {Object} context - ä¸Šä¸‹æ–‡ä¿¡æ¯
   * @returns {string} æ¨èçš„æ¨¡å‹åç§°
   */
  selectModel(taskType, context = {}) {
    // æ£€æµ‹ä¸­æ–‡è¾“å…¥
    if (context.text && /[\u4e00-\u9fa5]/.test(context.text)) {
      console.log('ğŸ‡¨ğŸ‡³ [MultiAI] æ£€æµ‹åˆ°ä¸­æ–‡è¾“å…¥ï¼Œè·¯ç”±åˆ°DeepSeek');
      return 'deepseek-chat';
    }

    // æ£€æµ‹æ˜¯å¦éœ€è¦å®æ—¶ä¿¡æ¯
    if (context.requiresRealTime || /æœ€æ–°|ä»Šå¤©|å®æ—¶|ç°åœ¨/.test(context.text || '')) {
      console.log('ğŸ” [MultiAI] éœ€è¦å®æ—¶ä¿¡æ¯ï¼Œè·¯ç”±åˆ°Perplexity');
      return 'sonar-pro';
    }

    // æ ¹æ®ä»»åŠ¡ç±»å‹è·¯ç”±
    const model = this.routingRules[taskType] || this.routingRules.default;
    console.log(`ğŸ§  [MultiAI] ä»»åŠ¡ç±»å‹: ${taskType}, é€‰æ‹©æ¨¡å‹: ${model}`);
    return model;
  }

  /**
   * ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£
   * @param {string} modelName - æ¨¡å‹åç§°
   * @param {Array} messages - å¯¹è¯æ¶ˆæ¯
   * @param {Object} options - è°ƒç”¨é€‰é¡¹
   * @returns {Promise<Object>} ç”Ÿæˆç»“æœ
   */
  async generate(modelName, messages, options = {}) {
    const startTime = Date.now();
    const modelConfig = this.models[modelName];
    
    if (!modelConfig) {
      throw new Error(`ä¸æ”¯æŒçš„æ¨¡å‹: ${modelName}`);
    }

    const provider = modelConfig.provider;
    const apiKey = this.apiKeys[provider];

    if (!apiKey) {
      console.warn(`âš ï¸  [MultiAI] ${provider} APIå¯†é’¥æœªé…ç½®ï¼Œé™çº§åˆ°é»˜è®¤æ¨¡å‹`);
      return this.fallbackGenerate(messages, options);
    }

    try {
      console.log(`ğŸš€ [MultiAI] è°ƒç”¨ ${modelName} (${provider})`);

      const response = await this.callProvider(
        provider,
        modelConfig.endpoint,
        apiKey,
        modelName,
        messages,
        options
      );

      // æˆæœ¬è¿½è¸ª
      const cost = this.trackCost(modelName, response.usage);
      const elapsed = Date.now() - startTime;

      console.log(`âœ… [MultiAI] ${modelName} å®Œæˆ (${elapsed}ms, $${cost.toFixed(4)})`);

      return {
        success: true,
        text: response.content,
        model: modelName,
        provider: provider,
        usage: response.usage,
        cost_usd: cost,
        elapsed_ms: elapsed,
        features: modelConfig.features
      };

    } catch (error) {
      console.error(`âŒ [MultiAI] ${modelName} è°ƒç”¨å¤±è´¥:`, error.message);
      
      // é™çº§å¤„ç†
      if (modelName !== 'gpt-4o-mini') {
        console.log(`ğŸ”„ [MultiAI] é™çº§åˆ°å¤‡ç”¨æ¨¡å‹`);
        return this.fallbackGenerate(messages, options);
      }
      
      throw error;
    }
  }

  /**
   * è°ƒç”¨ç‰¹å®šæä¾›å•†çš„API
   */
  async callProvider(provider, endpoint, apiKey, model, messages, options) {
    // Anthropic Claudeç‰¹æ®Šå¤„ç†
    if (provider === 'anthropic') {
      return this.callAnthropicAPI(endpoint, apiKey, model, messages, options);
    }

    // Google Geminiç‰¹æ®Šå¤„ç†
    if (provider === 'google') {
      return this.callGoogleAPI(endpoint, apiKey, model, messages, options);
    }

    // OpenAIå…¼å®¹æ ¼å¼ï¼ˆOpenAI, DeepSeek, Mistral, Perplexityï¼‰
    const requestBody = {
      model: model,
      messages: messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.maxTokens || 2048
    };

    // Perplexityç‰¹æ®Šå‚æ•°
    if (provider === 'perplexity') {
      requestBody.return_citations = true;
      requestBody.return_images = false;
    }

    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`APIè°ƒç”¨å¤±è´¥ (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    // ç»Ÿä¸€å“åº”æ ¼å¼
    return {
      content: data.choices[0].message.content,
      usage: {
        prompt_tokens: data.usage?.prompt_tokens || 0,
        completion_tokens: data.usage?.completion_tokens || 0,
        total_tokens: data.usage?.total_tokens || 0
      },
      citations: data.citations || [] // Perplexityä¸“å±
    };
  }

  /**
   * è°ƒç”¨Anthropic Claude API
   */
  async callAnthropicAPI(endpoint, apiKey, model, messages, options) {
    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const systemMessage = messages.find(msg => msg.role === 'system');
    const userMessages = messages.filter(msg => msg.role !== 'system');

    const requestBody = {
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: options.maxTokens || 4096,
      temperature: options.temperature || 0.7,
      messages: userMessages.map(msg => ({
        role: msg.role,
        content: msg.content
      }))
    };

    if (systemMessage) {
      requestBody.system = systemMessage.content;
    }

    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': apiKey,
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Claude APIè°ƒç”¨å¤±è´¥ (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    return {
      content: data.content[0].text,
      usage: {
        prompt_tokens: data.usage.input_tokens,
        completion_tokens: data.usage.output_tokens,
        total_tokens: data.usage.input_tokens + data.usage.output_tokens
      }
    };
  }

  /**
   * è°ƒç”¨Google Gemini API
   */
  async callGoogleAPI(endpoint, apiKey, model, messages, options) {
    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
    const contents = messages.filter(msg => msg.role !== 'system').map(msg => ({
      role: msg.role === 'user' ? 'user' : 'model',
      parts: [{ text: msg.content }]
    }));

    // SystemæŒ‡ä»¤æ”¾åœ¨generationConfigä¸­
    const systemInstruction = messages.find(msg => msg.role === 'system');

    const requestBody = {
      contents: contents,
      generationConfig: {
        temperature: options.temperature || 0.7,
        maxOutputTokens: options.maxTokens || 2048
      }
    };

    if (systemInstruction) {
      requestBody.systemInstruction = {
        parts: [{ text: systemInstruction.content }]
      };
    }

    const response = await fetch(`${endpoint}?key=${apiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Gemini APIè°ƒç”¨å¤±è´¥ (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    return {
      content: data.candidates[0].content.parts[0].text,
      usage: {
        prompt_tokens: data.usageMetadata?.promptTokenCount || 0,
        completion_tokens: data.usageMetadata?.candidatesTokenCount || 0,
        total_tokens: data.usageMetadata?.totalTokenCount || 0
      }
    };
  }

  /**
   * é™çº§åˆ°å¤‡ç”¨æ¨¡å‹ï¼ˆOpenAI GPT-4o-miniï¼‰
   */
  async fallbackGenerate(messages, options) {
    console.log('ğŸ›¡ï¸  [MultiAI] ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: gpt-4o-mini');
    return this.generate('gpt-4o-mini', messages, options);
  }

  /**
   * æˆæœ¬è¿½è¸ª
   */
  trackCost(modelName, usage) {
    const modelConfig = this.models[modelName];
    const inputCost = (usage.prompt_tokens / 1000) * modelConfig.costPer1kTokens.input;
    const outputCost = (usage.completion_tokens / 1000) * modelConfig.costPer1kTokens.output;
    const totalCost = inputCost + outputCost;

    // æ›´æ–°è¿½è¸ªæ•°æ®
    this.costTracking.totalCalls++;
    this.costTracking.totalTokens.input += usage.prompt_tokens;
    this.costTracking.totalTokens.output += usage.completion_tokens;
    this.costTracking.totalCost += totalCost;

    if (!this.costTracking.byModel[modelName]) {
      this.costTracking.byModel[modelName] = {
        calls: 0,
        tokens: { input: 0, output: 0 },
        cost: 0
      };
    }

    this.costTracking.byModel[modelName].calls++;
    this.costTracking.byModel[modelName].tokens.input += usage.prompt_tokens;
    this.costTracking.byModel[modelName].tokens.output += usage.completion_tokens;
    this.costTracking.byModel[modelName].cost += totalCost;

    return totalCost;
  }

  /**
   * è·å–æˆæœ¬ç»Ÿè®¡
   */
  getCostReport() {
    return {
      summary: {
        totalCalls: this.costTracking.totalCalls,
        totalTokens: this.costTracking.totalTokens.input + this.costTracking.totalTokens.output,
        totalCost: this.costTracking.totalCost
      },
      byModel: this.costTracking.byModel
    };
  }
}

// å•ä¾‹æ¨¡å¼
let instance = null;

function getMultiAIProvider() {
  if (!instance) {
    instance = new MultiAIProvider();
  }
  return instance;
}

module.exports = {
  MultiAIProvider,
  getMultiAIProvider
};
