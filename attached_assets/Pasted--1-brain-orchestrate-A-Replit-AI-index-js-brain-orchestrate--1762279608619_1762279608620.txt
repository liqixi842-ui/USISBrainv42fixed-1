【第1步】在 /brain/orchestrate 打点与校验
方案A · 发给 Replit AI 的自然语言指令

请在 index.js 的 /brain/orchestrate 处理函数里加入调试日志与显式调用链校验：

确保在解析到 text, mode, symbols, budget 之后，调用 calculateComplexityScore({ text, mode, symbols })，将返回结果保存到 complexity。

打印日志：[L1] complexity=..., tier=...。

调用 selectOptimalModels({ complexity, budget })，将返回的模型数组保存到 modelsChosen，并打印日志：[L2] budget=..., modelsChosen=[...]。

若 complexity.tier === 'L3'，打印 [L3] deep-reasoning enabled，并设置后续使用的推理路径（例如启用 o1/Claude Opus 流程的布尔开关）。

把以上三个字段（complexity、modelsChosen、tier）回填到响应 JSON 的 debug 字段中，形如：

"debug": { "complexity": { "score": 7.2, "tier": "L2" }, "budget":"medium", "modelsChosen":["gpt-4o-mini","claude-haiku"] }


保留原有逻辑不变（图表 actions、文字回复等），只是新增日志与 debug 字段。

日志统一用 console.info，带请求 id（若已有 request_id 则沿用）。

执行后请返回修改过的代码片段给我，或者直接贴运行日志截图（包含 [L1]/[L2]/[L3]）。

方案B · 代码最小补丁（直接粘到同位置手动改）

找到 app.post('/brain/orchestrate', async (req, res) => { ... })，在解析入参后、任何模型/图表决策之前，插入如下代码块；并在最终 res.json(...) 时附带 debug 字段。

// —— 放在入参解析之后 ——
// 假设已有：const { text, mode = 'auto', budget = 'medium', chat_type, user_id, symbols = [] } = req.body;
// 若 symbols 在别处解析，请按你实际变量名替换

// 1) 复杂度评分
const complexity = await calculateComplexityScore({ text, mode, symbols });
// 统一生成请求ID（若你已有 request_id 就沿用）
const requestId = req.request_id || crypto.randomUUID?.() || String(Date.now());

// 打点：L1
console.info(`[L1][${requestId}] complexity.score=${complexity?.score} tier=${complexity?.tier} mode=${mode} symbols=${JSON.stringify(symbols)}`);

// 2) 模型选择（受预算影响）
const modelsChosen = await selectOptimalModels({ complexity, budget });

// 打点：L2
console.info(`[L2][${requestId}] budget=${budget} modelsChosen=${JSON.stringify(modelsChosen)}`);

// 3) 深度推理开关
const enableDeepReasoning = complexity?.tier === 'L3';
if (enableDeepReasoning) {
  console.info(`[L3][${requestId}] deep-reasoning enabled (o1/Claude Opus path)`);
}

// ……此处保留你原有的业务：意图解析、数据源分流、actions 生成、图表等……
// 假设最终你准备了 responsePayload 作为返回体

// 4) 在响应里带上 debug 字段（仅开发期；若你有 prod 开关可按需隐藏）
responsePayload.debug = {
  requestId,
  complexity,
  budget,
  modelsChosen,
  deepReasoning: enableDeepReasoning
};

// 最终返回
return res.json(responsePayload);


注意：如果 calculateComplexityScore 或 selectOptimalModels 在别处已经调用，也保留原逻辑，这个补丁的目的就是显式打点与确定调用发生。重复调用会造成二次日志；若已确认原处在 orchestrate 内调用，可只添加日志与 debug 回填。

验证方法（立刻可测）

在 Replit 控制台观察日志（或你的日志面板）：

发送 “预览下宏观数据” → 预期：[L1] tier=L1；[L2] modelsChosen 为低成本模型；deepReasoning=false。

发送 “失业率上升了吗” → 预期：tier=L2，选择中档模型，并产生 send_chart action。

发送 “给我一份对CPI、GDP、失业率、利率的前瞻性场景推演，并结合历史衰退区间做风险敞口建议” → 预期：tier=L3，日志出现 [L3]，并选择 o1/Opus/等重模型（受 budget 影响）。

在 n8n 的执行记录里查看返回 JSON，确认 debug 字段随同响应返回（便于你不用切换窗口也能看见评分/选模结果）。

若某条请求未出现 [L1]/[L2] 日志，说明 orchestrate 没有真正走到评分与选模；需要把打点代码上移到最靠前的决策入口位置。

预期结果

你能清楚看到每次请求的 复杂度分、层级（L1/L2/L3）、预算、被选中的模型组合，以及是否触发 深度推理。

这一步不改变任何对外行为，只是把“有没有用上复杂度评分”的证据摊在桌面上。

完成后，把一段日志或返回 JSON 的 debug 字段贴给我，我马上带你做第2步：优化模型选择逻辑（让 budget 真正影响 modelsChosen，并输出“为什么选这些模型”的解释）。